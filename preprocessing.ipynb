{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import essay\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbti_to_big5(mbti):\n",
    "    # check https://en.wikipedia.org/wiki/Myers%E2%80%93Briggs_Type_Indicator\n",
    "    # in mbti (myers briggs) ther is invrovert vs. extrovert\n",
    "    # which corellates with Extroversion in BIG FIVE\n",
    "    mbti = mbti.lower()\n",
    "    cEXT, cNEU, cAGR, cCON, cOPN = 0,np.NaN,0,0,0\n",
    "    if mbti[0] == \"i\":\n",
    "        cEXT = 0\n",
    "    elif mbti[0] == \"e\":\n",
    "        cEXT = 1\n",
    "\n",
    "    # in mbti (myers briggs) ther is I*N*TUITION vs SENSING\n",
    "    # which corellates with OPENNESS in BIG FIVE\n",
    "    if mbti[1] == \"n\":\n",
    "        cOPN = 1\n",
    "    elif mbti[1] == \"s\":\n",
    "        cOPN = 0   \n",
    "\n",
    "    # in mbti (myers briggs) ther is THINKER vs FEELER\n",
    "    # which corellates with AGREEABLENESS in BIG FIVE\n",
    "    if mbti[2] == \"t\":\n",
    "        cAGR = 0\n",
    "    elif mbti[2] == \"f\":\n",
    "        cAGR = 1\n",
    "\n",
    "    # in mbti (myers briggs) ther is JUDGER vs PERCEIVER\n",
    "    # which corellates with CONSCIENTIOUSNESS in BIG FIVE (worst corellation)\n",
    "    # especially bec. orderlyness corellates with conscientiousness\n",
    "    if mbti[3] == \"p\":\n",
    "        cCON = 0\n",
    "    elif mbti[3] == \"j\":\n",
    "        cCON = 1\n",
    "\n",
    "    return cEXT, cNEU, cAGR, cCON, cOPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unemotional_sentences(emotional_words, text_as_one_string):\n",
    "    reduced_s = \"\"\n",
    "    sentences = re.split('(?<=[.!?]) +', text_as_one_string)\n",
    "    for s in sentences:\n",
    "        if any(e in s for e in emotional_words):\n",
    "            reduced_s = reduced_s + s + \" \"\n",
    "        else:\n",
    "            pass\n",
    "    return reduced_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply put every row of our read dataframe into a list of \n",
    "# the object \"Essay\"\n",
    "# remove data from list substract\n",
    "def create_essays(df, subtract=None):\n",
    "    essays = []\n",
    "    for index, row in df.iterrows():\n",
    "        essays.append(essay.Essay(row.TEXT, row.cEXT, row.cNEU, row.cAGR, row.cCON, row.cOPN))  \n",
    "\n",
    "    # remove scentences which do not contain emotionally charged words \n",
    "    # from the emotional lexicon\n",
    "    if subtract != None:\n",
    "        for x in essays:\n",
    "            x.filtered_text = remove_unemotional_sentences(emotional_words, x.clean_text)\n",
    "\n",
    "    return essays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the essays from paper\n",
    "## scientific gold standard\n",
    "## https://sentic.net/deep-learning-based-personality-detection.pdf\n",
    "### (scientific gold standard \"stream of counsciousness\" essays labeled with personality traits of the big five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well, right now I just woke up from a mid-day ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well, here we go with the stream of consciousn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An open keyboard and buttons to push. The thin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I can't believe it!  It's really happening!  M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, here I go with the good old stream of co...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>I'm home. wanted to go to bed but remembe...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>Stream of consiousnesssskdj. How do you s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>It is Wednesday, December 8th and a lot has be...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>Man this week has been hellish. Anyways, now i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>I have just gotten off the phone with brady. I...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2467 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   TEXT  cEXT  cNEU  cAGR   \n",
       "0     Well, right now I just woke up from a mid-day ...     0     1     1  \\\n",
       "1     Well, here we go with the stream of consciousn...     0     0     1   \n",
       "2     An open keyboard and buttons to push. The thin...     0     1     0   \n",
       "3     I can't believe it!  It's really happening!  M...     1     0     1   \n",
       "4     Well, here I go with the good old stream of co...     1     0     1   \n",
       "...                                                 ...   ...   ...   ...   \n",
       "2462       I'm home. wanted to go to bed but remembe...     0     1     0   \n",
       "2463       Stream of consiousnesssskdj. How do you s...     1     1     0   \n",
       "2464  It is Wednesday, December 8th and a lot has be...     0     0     1   \n",
       "2465  Man this week has been hellish. Anyways, now i...     0     1     0   \n",
       "2466  I have just gotten off the phone with brady. I...     0     1     1   \n",
       "\n",
       "      cCON  cOPN  \n",
       "0        0     1  \n",
       "1        0     0  \n",
       "2        1     1  \n",
       "3        1     0  \n",
       "4        0     1  \n",
       "...    ...   ...  \n",
       "2462     1     0  \n",
       "2463     0     1  \n",
       "2464     0     0  \n",
       "2465     0     1  \n",
       "2466     0     1  \n",
       "\n",
       "[2467 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we read in the data from \"essays.csv\" and \n",
    "# \"essays.csv\" contains all essays with classification \n",
    "df_essays = pd.read_csv('data/training/essays.csv', encoding='cp1252', delimiter=',', quotechar='\"')\n",
    "\n",
    "# for every essay, we replace the personalitiy categories \n",
    "# of the essay wich are \"y\" and \"n\" with \"1\" and \"0\" \n",
    "for e in df_essays.columns[2:7]:\n",
    "    df_essays[e] = df_essays[e].replace('n', '0')\n",
    "    df_essays[e] = df_essays[e].replace('y', '1')\n",
    "    # not sure if we need this line: furter investigation possible:\n",
    "    df_essays[e] = pd.to_numeric(df_essays[e])\n",
    "\n",
    "df_essays = df_essays[[\"TEXT\", \"cEXT\", \"cNEU\", \"cAGR\", \"cCON\", \"cOPN\"]]\n",
    "df_essays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the MBTI kaggle dataset\n",
    "## https://www.kaggle.com/datasnaek/mbti-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw ht...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'You're fired. That's another silly misconcept...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>'https://www.youtube.com/watch?v=t8edHB_h908 I...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>'So...if this thread already exists someplace ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>'So many questions when i do these things.  I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>'I am very conflicted right now when it comes ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>'It has been too long since I have been on per...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8675 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   TEXT  cEXT  cNEU  cAGR   \n",
       "0     'http://www.youtube.com/watch?v=qsXHcwe3krw ht...     0   NaN     1  \\\n",
       "1     'I'm finding the lack of me in these posts ver...     1   NaN     0   \n",
       "2     'Good one  _____   https://www.youtube.com/wat...     0   NaN     0   \n",
       "3     'Dear INTP,   I enjoyed our conversation the o...     0   NaN     0   \n",
       "4     'You're fired. That's another silly misconcept...     1   NaN     0   \n",
       "...                                                 ...   ...   ...   ...   \n",
       "8670  'https://www.youtube.com/watch?v=t8edHB_h908 I...     0   NaN     1   \n",
       "8671  'So...if this thread already exists someplace ...     1   NaN     1   \n",
       "8672  'So many questions when i do these things.  I ...     0   NaN     0   \n",
       "8673  'I am very conflicted right now when it comes ...     0   NaN     1   \n",
       "8674  'It has been too long since I have been on per...     0   NaN     1   \n",
       "\n",
       "      cCON  cOPN  \n",
       "0        1     1  \n",
       "1        0     1  \n",
       "2        0     1  \n",
       "3        1     1  \n",
       "4        1     1  \n",
       "...    ...   ...  \n",
       "8670     0     0  \n",
       "8671     0     1  \n",
       "8672     0     1  \n",
       "8673     0     1  \n",
       "8674     0     1  \n",
       "\n",
       "[8675 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kaggle = pd.read_csv('data/training/mbti_1.csv',  skiprows=0 )\n",
    "\n",
    "df_kaggle[\"cEXT\"] =   df_kaggle.apply(lambda x: mbti_to_big5(x.type)[0], 1)\n",
    "df_kaggle[\"cNEU\"] =   df_kaggle.apply(lambda x: mbti_to_big5(x.type)[1], 1)\n",
    "df_kaggle[\"cAGR\"] =   df_kaggle.apply(lambda x: mbti_to_big5(x.type)[2], 1)\n",
    "df_kaggle[\"cCON\"] =   df_kaggle.apply(lambda x: mbti_to_big5(x.type)[3], 1)\n",
    "df_kaggle[\"cOPN\"] =   df_kaggle.apply(lambda x: mbti_to_big5(x.type)[4], 1)\n",
    "\n",
    "df_kaggle = df_kaggle[[\"posts\", \"cEXT\", \"cNEU\", \"cAGR\", \"cCON\", \"cOPN\"]]\n",
    "df_kaggle.columns = [\"TEXT\", \"cEXT\", \"cNEU\", \"cAGR\", \"cCON\", \"cOPN\"]\n",
    "\n",
    "# remove som fancy ||| things\n",
    "df_kaggle[\"TEXT\"] = df_kaggle.apply(lambda x: x.TEXT.replace(\"|||\", \" \")[:], 1)\n",
    "\n",
    "\n",
    "df_kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Reddit Dataset\n",
    "## Matej Gjurković  https://peopleswksh.github.io/pdf/PEOPLES12.pdf\n",
    "### received on request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/training/typed_comments.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# the file is kinda huge. thanks Matej\u001b[39;00m\n\u001b[1;32m      2\u001b[0m file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata/training/typed_comments.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m df_reddit \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(file)\n",
      "File \u001b[0;32m~/Downloads/Project-Video_copy/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/Downloads/Project-Video_copy/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Downloads/Project-Video_copy/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/Downloads/Project-Video_copy/venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Downloads/Project-Video_copy/venv/lib/python3.9/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/training/typed_comments.csv'"
     ]
    }
   ],
   "source": [
    "# the file is kinda huge. thanks Matej\n",
    "file = 'data/training/typed_comments.csv'\n",
    "df_reddit = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_reddit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#remove some rows to to keep longer text (the 420 because it makes avg word count compareable to the rest of the data)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_reddit \u001b[39m=\u001b[39m df_reddit[df_reddit\u001b[39m.\u001b[39mword_count \u001b[39m>\u001b[39m \u001b[39m420\u001b[39m]\n\u001b[1;32m      4\u001b[0m df_reddit[\u001b[39m\"\u001b[39m\u001b[39mcEXT\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m   df_reddit\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: mbti_to_big5(x\u001b[39m.\u001b[39mtype)[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m df_reddit[\u001b[39m\"\u001b[39m\u001b[39mcNEU\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m   df_reddit\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: mbti_to_big5(x\u001b[39m.\u001b[39mtype)[\u001b[39m1\u001b[39m], \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_reddit' is not defined"
     ]
    }
   ],
   "source": [
    "#remove some rows to to keep longer text (the 420 because it makes avg word count compareable to the rest of the data)\n",
    "df_reddit = df_reddit[df_reddit.word_count > 420]\n",
    "\n",
    "df_reddit[\"cEXT\"] =   df_reddit.apply(lambda x: mbti_to_big5(x.type)[0], 1)\n",
    "df_reddit[\"cNEU\"] =   df_reddit.apply(lambda x: mbti_to_big5(x.type)[1], 1)\n",
    "df_reddit[\"cAGR\"] =   df_reddit.apply(lambda x: mbti_to_big5(x.type)[2], 1)\n",
    "df_reddit[\"cCON\"] =   df_reddit.apply(lambda x: mbti_to_big5(x.type)[3], 1)\n",
    "df_reddit[\"cOPN\"] =   df_reddit.apply(lambda x: mbti_to_big5(x.type)[4], 1)\n",
    "df_reddit = df_reddit[[\"comment\", \"cEXT\", \"cNEU\", \"cAGR\", \"cCON\", \"cOPN\"]]\n",
    "df_reddit.columns = [\"TEXT\", \"cEXT\", \"cNEU\", \"cAGR\", \"cCON\", \"cOPN\"]\n",
    "df_reddit.reset_index(drop=True, inplace=True)\n",
    "df_reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Emotional Lexicon to substract from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also from \"Emotional_Lexicon.csv\" we read in the data, which is a list of words and \n",
    "# has several categories of emotions. \n",
    "# anger - anticipation - disgust - fear - joy - negative - positive \n",
    "# - sadness - surprise - trust - Charged\n",
    "df_lexicon = pd.read_csv('data/training/Emotion_Lexicon.csv', index_col=0)\n",
    "\n",
    "\n",
    "# some of the words have no emotional category, \n",
    "# so let's remove them as they have no use to us.\n",
    "# can be improved by not even loading them when all columns are 0. maybe later.\n",
    "df_lexicon = df_lexicon[(df_lexicon.T != 0).any()]\n",
    "emotional_words = df_lexicon.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatinate the datasets for 3 different data to work with and compare to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Base 1 - only esseys.csv - and save as object list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved entries:  2467\n"
     ]
    }
   ],
   "source": [
    "# save preprocessed data by converting into OBJECT essay and save with pickle and removing non emotional scentences\n",
    "essays = create_essays(df_essays, emotional_words)\n",
    "pickle.dump(essays, open( \"essays/essays2467.p\", \"wb\"))\n",
    "print(\"saved entries: \", len(essays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Base 2 - Essay Data and Kaggle data - and save as object list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved entries:  11142\n"
     ]
    }
   ],
   "source": [
    "# concatinate the dataframes:\n",
    "frames  = [df_essays, df_kaggle]\n",
    "essays_kaggle = pd.concat(frames, sort=False)\n",
    "essays_kaggle.reset_index(drop=True)\n",
    "\n",
    "# preprocess data by converting into OBJECT essay and save with pickle and removing non emotional scentences\n",
    "essays_kaggle = create_essays(essays_kaggle, emotional_words)\n",
    "pickle.dump(essays_kaggle, open(\"essays/essays11142.p\", \"wb\"))\n",
    "print(\"saved entries: \", len(essays_kaggle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Base 3 - Essay Data and Kaggle data and Reddit data - and save as object list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved entries:  89364\n"
     ]
    }
   ],
   "source": [
    "# concatinate the dataframes:\n",
    "frames  = [df_essays, df_kaggle, df_reddit]\n",
    "essays_kaggle_reddit = pd.concat(frames, sort=False)\n",
    "essays_kaggle_reddit.reset_index(drop=True)\n",
    "\n",
    "# preprocess data by converting into OBJECT essay and save with pickle and removing non emotional scentences\n",
    "essays_kaggle_reddit = create_essays(essays_kaggle_reddit, emotional_words)\n",
    "pickle.dump(essays_kaggle_reddit, open(\"essays/essays89364.p\", \"wb\"))\n",
    "print(\"saved entries: \", len(essays_kaggle_reddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
