{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model based on Bags of Words feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import essay\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded count of essays: 11142\n"
     ]
    }
   ],
   "source": [
    "# load the preprocessed data which we saved\n",
    "# choose how much data you want to load (2467, 11142 or 89364)\n",
    "\n",
    "#essays = pickle.load(open( \"data/essays/essays2467.p\", \"rb\"))\n",
    "\n",
    "essays = pickle.load(open( \"data/essays/essays11142.p\", \"rb\"))\n",
    "\n",
    "#essays = pickle.load(open( \"data/essays/essays89364.p\", \"rb\"))\n",
    "\n",
    "print(\"loaded count of essays:\", len(essays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data in train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "training, test = train_test_split(essays, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [x.clean_text for x in training]\n",
    "\n",
    "train_y_cEXT = [x.cEXT for x in training]\n",
    "train_y_cNEU = [x.cNEU for x in training]\n",
    "train_y_cAGR = [x.cAGR for x in training]\n",
    "train_y_cCON = [x.cCON for x in training]\n",
    "train_y_cOPN = [x.cOPN for x in training]\n",
    "\n",
    "\n",
    "test_x = [x.clean_text for x in test]\n",
    "\n",
    "test_y_cEXT = [x.cEXT for x in test]\n",
    "test_y_cNEU = [x.cNEU for x in test]\n",
    "test_y_cAGR = [x.cAGR for x in test]\n",
    "test_y_cCON = [x.cCON for x in test]\n",
    "test_y_cOPN = [x.cOPN for x in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bags of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "# create vectors from our words\n",
    "train_x_vectors = bow_vectorizer.fit_transform(train_x)\n",
    "test_x_vectors = bow_vectorizer.transform(test_x)\n",
    "# # now that's a big thing :-O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for evaluation save some data for later:\n",
    "evaluation = []\n",
    "data = len(essays)\n",
    "vec_name = \"BoW\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Extraversion cEXT using SVM...\n",
      "cEXT score:  0.7344100493494841\n",
      "training Neuroticism cNEU using SVM...\n",
      "with this data not available (MBTI only 4 dimensions)\n",
      "training Agreeableness cAGR using using SVM...\n",
      "cAGR score:  0.7128757290264692\n",
      "training Conscientiousness cCON using SVM...\n",
      "cCON score:  0.6810228802153432\n",
      "training Openness to Experience cOPN using SVM...\n",
      "cOPN score:  0.7707492148945716\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "name = \"svm\"\n",
    "\n",
    "print(\"training Extraversion cEXT using SVM...\")\n",
    "clf_svm_cEXT = svm.SVC(kernel='linear')\n",
    "clf_svm_cEXT.fit(train_x_vectors, train_y_cEXT)\n",
    "evaluation.append([data, vec_name, name, \"cEXT\", clf_svm_cEXT.score(test_x_vectors, test_y_cEXT)])\n",
    "print(\"cEXT score: \", clf_svm_cEXT.score(test_x_vectors, test_y_cEXT))\n",
    "\n",
    "try:\n",
    "    print(\"training Neuroticism cNEU using SVM...\")\n",
    "    clf_svm_cNEU = svm.SVC(kernel='linear')\n",
    "    clf_svm_cNEU.fit(train_x_vectors, train_y_cNEU)\n",
    "    evaluation.append([data, vec_name, name, \"cNEU\", clf_svm_cNEU.score(test_x_vectors, test_y_cNEU)])\n",
    "    print(\"cNEU score: \", clf_svm_cNEU.score(test_x_vectors, test_y_cNEU))\n",
    "except:\n",
    "    print(\"with this data not available (MBTI only 4 dimensions)\")\n",
    "    \n",
    "print(\"training Agreeableness cAGR using using SVM...\")\n",
    "clf_svm_cAGR = svm.SVC(kernel='linear')\n",
    "clf_svm_cAGR.fit(train_x_vectors, train_y_cAGR)\n",
    "evaluation.append([data, vec_name, name, \"cAGR\", clf_svm_cAGR.score(test_x_vectors, test_y_cAGR)])\n",
    "\n",
    "print(\"cAGR score: \", clf_svm_cAGR.score(test_x_vectors, test_y_cAGR))\n",
    "\n",
    "print(\"training Conscientiousness cCON using SVM...\")\n",
    "clf_svm_cCON = svm.SVC(kernel='linear')\n",
    "clf_svm_cCON.fit(train_x_vectors, train_y_cCON)\n",
    "evaluation.append([data, vec_name, name, \"cCON\", clf_svm_cCON.score(test_x_vectors, test_y_cCON)])\n",
    "print(\"cCON score: \", clf_svm_cCON.score(test_x_vectors, test_y_cCON))\n",
    "\n",
    "print(\"training Openness to Experience cOPN using SVM...\")\n",
    "clf_svm_cOPN = svm.SVC(kernel='linear')\n",
    "clf_svm_cOPN.fit(train_x_vectors, train_y_cOPN)\n",
    "evaluation.append([data, vec_name, name, \"cOPN\", clf_svm_cOPN.score(test_x_vectors, test_y_cOPN)])\n",
    "print(\"cOPN score: \", clf_svm_cOPN.score(test_x_vectors, test_y_cOPN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Extraversion cEXT using XGBoost...\n",
      "cEXT score:  0.7801704800358905\n",
      "Training Neuroticism cNEU using XGBoost...\n",
      "with this data not available (MBTI only 4 dimensions)\n",
      "Training Agreeableness cAGR using XGBoost...\n",
      "cAGR score:  0.7622252131000449\n",
      "Training Conscientiousness cCON using XGBoost...\n",
      "cCON score:  0.7267833109017496\n",
      "Training Openness to Experience cOPN using XGBoost...\n",
      "cOPN score:  0.8268281740690893\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_classifier = XGBClassifier()\n",
    "\n",
    "# Train and evaluate for Extraversion (cEXT)\n",
    "print(\"Training Extraversion cEXT using XGBoost...\")\n",
    "clf_xgb_cEXT = XGBClassifier()\n",
    "clf_xgb_cEXT.fit(train_x_vectors, train_y_cEXT)\n",
    "accuracy_cEXT = clf_xgb_cEXT.score(test_x_vectors, test_y_cEXT)\n",
    "evaluation.append([data, vec_name, \"XGBoost\", \"cEXT\", accuracy_cEXT])\n",
    "print(\"cEXT score: \", accuracy_cEXT)\n",
    "\n",
    "try:\n",
    "    # Train and evaluate for Neuroticism (cNEU)\n",
    "    print(\"Training Neuroticism cNEU using XGBoost...\")\n",
    "    clf_xgb_cNEU = XGBClassifier()\n",
    "    clf_xgb_cNEU.fit(train_x_vectors, train_y_cNEU)\n",
    "    accuracy_cNEU = clf_xgb_cNEU.score(test_x_vectors, test_y_cNEU)\n",
    "    evaluation.append([data, vec_name, \"XGBoost\", \"cNEU\", accuracy_cNEU])\n",
    "    print(\"cNEU score: \", accuracy_cNEU)\n",
    "except:\n",
    "    print(\"with this data not available (MBTI only 4 dimensions)\")\n",
    "\n",
    "# Train and evaluate for Agreeableness (cAGR)\n",
    "print(\"Training Agreeableness cAGR using XGBoost...\")\n",
    "clf_xgb_cAGR = XGBClassifier()\n",
    "clf_xgb_cAGR.fit(train_x_vectors, train_y_cAGR)\n",
    "accuracy_cAGR = clf_xgb_cAGR.score(test_x_vectors, test_y_cAGR)\n",
    "evaluation.append([data, vec_name, \"XGBoost\", \"cAGR\", accuracy_cAGR])\n",
    "print(\"cAGR score: \", accuracy_cAGR)\n",
    "\n",
    "# Train and evaluate for Conscientiousness (cCON)\n",
    "print(\"Training Conscientiousness cCON using XGBoost...\")\n",
    "clf_xgb_cCON = XGBClassifier()\n",
    "clf_xgb_cCON.fit(train_x_vectors, train_y_cCON)\n",
    "accuracy_cCON = clf_xgb_cCON.score(test_x_vectors, test_y_cCON)\n",
    "evaluation.append([data, vec_name, \"XGBoost\", \"cCON\", accuracy_cCON])\n",
    "print(\"cCON score: \", accuracy_cCON)\n",
    "\n",
    "# Train and evaluate for Openness to Experience (cOPN)\n",
    "print(\"Training Openness to Experience cOPN using XGBoost...\")\n",
    "clf_xgb_cOPN = XGBClassifier()\n",
    "clf_xgb_cOPN.fit(train_x_vectors, train_y_cOPN)\n",
    "accuracy_cOPN = clf_xgb_cOPN.score(test_x_vectors, test_y_cOPN)\n",
    "evaluation.append([data, vec_name, \"XGBoost\", \"cOPN\", accuracy_cOPN])\n",
    "print(\"cOPN score: \", accuracy_cOPN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Extraversion cEXT using dec...\n",
      "cEXT score:  0.7272319425751458\n",
      "training Neuroticism cNEU using dec...\n",
      "with this data not available (MBTI only 4 dimensions)\n",
      "training Agreeableness cAGR using using dec...\n",
      "cAGR score:  0.6711529834006281\n",
      "training Conscientiousness cCON using dec...\n",
      "cCON score:  0.6428891879766712\n",
      "training Openness to Experience cOPN using dec...\n",
      "cOPN score:  0.7698519515477793\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "name = \"tree\"\n",
    "\n",
    "print(\"training Extraversion cEXT using dec...\")\n",
    "clf_dec_cEXT = tree.DecisionTreeClassifier()\n",
    "clf_dec_cEXT.fit(train_x_vectors, train_y_cEXT)\n",
    "evaluation.append([data, vec_name, name, \"cEXT\", clf_dec_cEXT.score(test_x_vectors, test_y_cEXT)])\n",
    "\n",
    "print(\"cEXT score: \", clf_dec_cEXT.score(test_x_vectors, test_y_cEXT))\n",
    "\n",
    "try:\n",
    "    print(\"training Neuroticism cNEU using dec...\")\n",
    "    clf_dec_cNEU = tree.DecisionTreeClassifier()\n",
    "    clf_dec_cNEU.fit(train_x_vectors, train_y_cNEU)\n",
    "    evaluation.append([data, vec_name, name, \"cNEU\", clf_dec_cNEU.score(test_x_vectors, test_y_cNEU)])\n",
    "    print(\"cNEU score: \", clf_dec_cNEU.score(test_x_vectors, test_y_cNEU))\n",
    "except:\n",
    "    print(\"with this data not available (MBTI only 4 dimensions)\")\n",
    "\n",
    "print(\"training Agreeableness cAGR using using dec...\")\n",
    "clf_dec_cAGR = tree.DecisionTreeClassifier()\n",
    "clf_dec_cAGR.fit(train_x_vectors, train_y_cAGR)\n",
    "evaluation.append([data, vec_name, name, \"cAGR\", clf_dec_cAGR.score(test_x_vectors, test_y_cAGR)])\n",
    "print(\"cAGR score: \", clf_dec_cAGR.score(test_x_vectors, test_y_cAGR))\n",
    "\n",
    "print(\"training Conscientiousness cCON using dec...\")\n",
    "clf_dec_cCON = tree.DecisionTreeClassifier()\n",
    "clf_dec_cCON.fit(train_x_vectors, train_y_cCON)\n",
    "evaluation.append([data, vec_name, name, \"cCON\", clf_dec_cCON.score(test_x_vectors, test_y_cCON)])\n",
    "print(\"cCON score: \", clf_dec_cCON.score(test_x_vectors, test_y_cCON))\n",
    "\n",
    "print(\"training Openness to Experience cOPN using dec...\")\n",
    "clf_dec_cOPN = tree.DecisionTreeClassifier()\n",
    "clf_dec_cOPN.fit(train_x_vectors, train_y_cOPN)\n",
    "evaluation.append([data, vec_name, name, \"cOPN\", clf_dec_cOPN.score(test_x_vectors, test_y_cOPN)])\n",
    "print(\"cOPN score: \", clf_dec_cOPN.score(test_x_vectors, test_y_cOPN))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Extraversion cEXT using GaussianNaiveBayes...\n",
      "cEXT score:  0.7079407806191117\n",
      "training Neuroticism cNEU using GaussianNaiveBayes...\n",
      "with this data not available (MBTI only 4 dimensions)\n",
      "training Agreeableness cAGR using using GaussianNaiveBayes...\n",
      "cAGR score:  0.5612382234185733\n",
      "training Conscientiousness cCON using GaussianNaiveBayes...\n",
      "cCON score:  0.5697622252131\n",
      "training Openness to Experience cOPN using GaussianNaiveBayes...\n",
      "cOPN score:  0.7837595334230597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "name = \"gNB\"\n",
    "# clf_gnb = GaussianNB()\n",
    "# clf_gnb.fit(train_x_vectors.toarray(), train_y)\n",
    "\n",
    "\n",
    "print(\"training Extraversion cEXT using GaussianNaiveBayes...\")\n",
    "clf_gnb_cEXT = GaussianNB()\n",
    "clf_gnb_cEXT.fit(train_x_vectors.toarray(), train_y_cEXT)\n",
    "evaluation.append([data, vec_name, name, \"cEXT\", clf_gnb_cEXT.score(test_x_vectors.toarray(), test_y_cEXT)])\n",
    "print(\"cEXT score: \", clf_gnb_cEXT.score(test_x_vectors.toarray(), test_y_cEXT))\n",
    "\n",
    "try:\n",
    "    print(\"training Neuroticism cNEU using GaussianNaiveBayes...\")\n",
    "    clf_gnb_cNEU = GaussianNB()\n",
    "    clf_gnb_cNEU.fit(train_x_vectors.toarray(), train_y_cNEU)\n",
    "    evaluation.append([data, vec_name, name, \"cNEU\", clf_gnb_cNEU.score(test_x_vectors.toarray(), test_y_cNEU)])\n",
    "    print(\"cNEU score: \", clf_gnb_cNEU.score(test_x_vectors.toarray(), test_y_cNEU))\n",
    "except:\n",
    "    print(\"with this data not available (MBTI only 4 dimensions)\")\n",
    "\n",
    "    \n",
    "print(\"training Agreeableness cAGR using using GaussianNaiveBayes...\")\n",
    "clf_gnb_cAGR = GaussianNB()\n",
    "clf_gnb_cAGR.fit(train_x_vectors.toarray(), train_y_cAGR)\n",
    "evaluation.append([data, vec_name, name, \"cAGR\", clf_gnb_cAGR.score(test_x_vectors.toarray(), test_y_cAGR)])\n",
    "print(\"cAGR score: \", clf_gnb_cAGR.score(test_x_vectors.toarray(), test_y_cAGR))\n",
    "\n",
    "print(\"training Conscientiousness cCON using GaussianNaiveBayes...\")\n",
    "clf_gnb_cCON = GaussianNB()\n",
    "clf_gnb_cCON.fit(train_x_vectors.toarray(), train_y_cCON)\n",
    "evaluation.append([data, vec_name, name, \"cCON\", clf_gnb_cCON.score(test_x_vectors.toarray(), test_y_cCON)])\n",
    "print(\"cCON score: \", clf_gnb_cCON.score(test_x_vectors.toarray(), test_y_cCON))\n",
    "\n",
    "print(\"training Openness to Experience cOPN using GaussianNaiveBayes...\")\n",
    "clf_gnb_cOPN = GaussianNB()\n",
    "clf_gnb_cOPN.fit(train_x_vectors.toarray(), train_y_cOPN)\n",
    "evaluation.append([data, vec_name, name, \"cOPN\", clf_gnb_cOPN.score(test_x_vectors.toarray(), test_y_cOPN)])\n",
    "print(\"cOPN score: \", clf_gnb_cOPN.score(test_x_vectors.toarray(), test_y_cOPN))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logisic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Extraversion cEXT using Logistic Regression...\n",
      "cEXT score:  0.7590847913862718\n",
      "training Neuroticism cNEU using Logistic Regression...\n",
      "with this data not available (MBTI only 4 dimensions)\n",
      "training Agreeableness cAGR using using Logistic Regression...\n",
      "cAGR score:  0.7357559443696725\n",
      "training Conscientiousness cCON using Logistic Regression...\n",
      "cCON score:  0.7034544638851503\n",
      "training Openness to Experience cOPN using Logistic Regression...\n",
      "cOPN score:  0.7900403768506057\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "name=\"logR\"\n",
    "print(\"training Extraversion cEXT using Logistic Regression...\")\n",
    "clf_log_cEXT = LogisticRegression(solver=\"newton-cg\")\n",
    "clf_log_cEXT.fit(train_x_vectors, train_y_cEXT)\n",
    "evaluation.append([data, vec_name, name, \"cEXT\", clf_log_cEXT.score(test_x_vectors, test_y_cEXT)])\n",
    "print(\"cEXT score: \", clf_log_cEXT.score(test_x_vectors, test_y_cEXT))\n",
    "\n",
    "try:\n",
    "    print(\"training Neuroticism cNEU using Logistic Regression...\")\n",
    "    clf_log_cNEU = LogisticRegression(solver=\"newton-cg\")\n",
    "    clf_log_cNEU.fit(train_x_vectors, train_y_cNEU)\n",
    "    evaluation.append([data, vec_name, name, \"cNEU\", clf_log_cNEU.score(test_x_vectors, test_y_cNEU)])\n",
    "    print(\"cNEU score: \", clf_log_cNEU.score(test_x_vectors, test_y_cNEU))\n",
    "except:\n",
    "    print(\"with this data not available (MBTI only 4 dimensions)\")\n",
    "    \n",
    "print(\"training Agreeableness cAGR using using Logistic Regression...\")\n",
    "clf_log_cAGR = LogisticRegression(solver=\"newton-cg\")\n",
    "clf_log_cAGR.fit(train_x_vectors, train_y_cAGR)\n",
    "evaluation.append([data, vec_name, name, \"cAGR\", clf_log_cAGR.score(test_x_vectors, test_y_cAGR)])\n",
    "print(\"cAGR score: \", clf_log_cAGR.score(test_x_vectors, test_y_cAGR))\n",
    "\n",
    "print(\"training Conscientiousness cCON using Logistic Regression...\")\n",
    "clf_log_cCON = LogisticRegression(solver=\"newton-cg\")\n",
    "clf_log_cCON.fit(train_x_vectors, train_y_cCON)\n",
    "evaluation.append([data, vec_name, name, \"cCON\", clf_log_cCON.score(test_x_vectors, test_y_cCON)])\n",
    "print(\"cCON score: \", clf_log_cCON.score(test_x_vectors, test_y_cCON))\n",
    "\n",
    "print(\"training Openness to Experience cOPN using Logistic Regression...\")\n",
    "clf_log_cOPN = LogisticRegression(solver=\"newton-cg\")\n",
    "clf_log_cOPN.fit(train_x_vectors, train_y_cOPN)\n",
    "evaluation.append([data, vec_name, name, \"cOPN\", clf_log_cOPN.score(test_x_vectors, test_y_cOPN)])\n",
    "print(\"cOPN score: \", clf_log_cOPN.score(test_x_vectors, test_y_cOPN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Extraversion cEXT using Random Forest...\n",
      "cEXT score:  0.7231942575145806\n",
      "training Neuroticism cNEU using Random Forest...\n",
      "with this data not available (MBTI only 4 dimensions)\n",
      "training Agreeableness cAGR using using Random Forest...\n",
      "cAGR score:  0.7016599371915657\n",
      "training Conscientiousness cCON using Random Forest...\n",
      "cCON score:  0.5872588604755495\n",
      "training Openness to Experience cOPN using Random Forest...\n",
      "cOPN score:  0.7819650067294751\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "name=\"RF\"\n",
    "\n",
    "\n",
    "print(\"training Extraversion cEXT using Random Forest...\")\n",
    "clf_rf_cEXT = RandomForestClassifier(n_estimators=100)\n",
    "clf_rf_cEXT.fit(train_x_vectors, train_y_cEXT)\n",
    "evaluation.append([data, vec_name, name, \"cEXT\", clf_rf_cEXT.score(test_x_vectors, test_y_cEXT)])\n",
    "print(\"cEXT score: \", clf_rf_cEXT.score(test_x_vectors, test_y_cEXT))\n",
    "\n",
    "try:\n",
    "    print(\"training Neuroticism cNEU using Random Forest...\")\n",
    "    clf_rf_cNEU = RandomForestClassifier(n_estimators=100)\n",
    "    clf_rf_cNEU.fit(train_x_vectors, train_y_cNEU)\n",
    "    evaluation.append([data, vec_name, name, \"cNEU\", clf_rf_cNEU.score(test_x_vectors, test_y_cNEU)])\n",
    "    print(\"cNEU score: \", clf_rf_cNEU.score(test_x_vectors, test_y_cNEU))\n",
    "except:\n",
    "    print(\"with this data not available (MBTI only 4 dimensions)\")\n",
    "\n",
    "print(\"training Agreeableness cAGR using using Random Forest...\")\n",
    "clf_rf_cAGR = RandomForestClassifier(n_estimators=100)\n",
    "clf_rf_cAGR.fit(train_x_vectors, train_y_cAGR)\n",
    "evaluation.append([data, vec_name, name, \"cAGR\", clf_rf_cAGR.score(test_x_vectors, test_y_cAGR)])\n",
    "print(\"cAGR score: \", clf_rf_cAGR.score(test_x_vectors, test_y_cAGR))\n",
    "\n",
    "print(\"training Conscientiousness cCON using Random Forest...\")\n",
    "clf_rf_cCON = RandomForestClassifier(n_estimators=100)\n",
    "clf_rf_cCON.fit(train_x_vectors, train_y_cCON)\n",
    "evaluation.append([data, vec_name, name, \"cCON\", clf_rf_cCON.score(test_x_vectors, test_y_cCON)])\n",
    "print(\"cCON score: \", clf_rf_cCON.score(test_x_vectors, test_y_cCON))\n",
    "\n",
    "print(\"training Openness to Experience cOPN using Random Forest...\")\n",
    "clf_rf_cOPN = RandomForestClassifier(n_estimators=100)\n",
    "clf_rf_cOPN.fit(train_x_vectors, train_y_cOPN)\n",
    "evaluation.append([data, vec_name, name, \"cOPN\", clf_rf_cOPN.score(test_x_vectors, test_y_cOPN)])\n",
    "print(\"cOPN score: \", clf_rf_cOPN.score(test_x_vectors, test_y_cOPN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation saved as data/evaluation/evaluation11142BoW.p\n"
     ]
    }
   ],
   "source": [
    "filename = \"data/evaluation/evaluation\" + str(data) + vec_name + \".p\"\n",
    "pickle.dump(evaluation, open(filename, \"wb\"))\n",
    "print(\"evaluation saved as\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11142, 'BoW', 'svm', 'cEXT', 0.7344100493494841], [11142, 'BoW', 'svm', 'cAGR', 0.7128757290264692], [11142, 'BoW', 'svm', 'cCON', 0.6810228802153432], [11142, 'BoW', 'svm', 'cOPN', 0.7707492148945716], [11142, 'BoW', 'XGBoost', 'cEXT', 0.7801704800358905], [11142, 'BoW', 'XGBoost', 'cAGR', 0.7622252131000449], [11142, 'BoW', 'XGBoost', 'cCON', 0.7267833109017496], [11142, 'BoW', 'XGBoost', 'cOPN', 0.8268281740690893], [11142, 'BoW', 'tree', 'cEXT', 0.7272319425751458], [11142, 'BoW', 'tree', 'cAGR', 0.6711529834006281], [11142, 'BoW', 'tree', 'cCON', 0.6428891879766712], [11142, 'BoW', 'tree', 'cOPN', 0.7698519515477793], [11142, 'BoW', 'gNB', 'cEXT', 0.7079407806191117], [11142, 'BoW', 'gNB', 'cAGR', 0.5612382234185733], [11142, 'BoW', 'gNB', 'cCON', 0.5697622252131], [11142, 'BoW', 'gNB', 'cOPN', 0.7837595334230597], [11142, 'BoW', 'logR', 'cEXT', 0.7590847913862718], [11142, 'BoW', 'logR', 'cAGR', 0.7357559443696725], [11142, 'BoW', 'logR', 'cCON', 0.7034544638851503], [11142, 'BoW', 'logR', 'cOPN', 0.7900403768506057], [11142, 'BoW', 'RF', 'cEXT', 0.7231942575145806], [11142, 'BoW', 'RF', 'cAGR', 0.7016599371915657], [11142, 'BoW', 'RF', 'cCON', 0.5872588604755495], [11142, 'BoW', 'RF', 'cOPN', 0.7819650067294751]]\n"
     ]
    }
   ],
   "source": [
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My try towards BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avishkar/Downloads/Project-Video_copy/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-15 06:11:36.870728: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 134s 756ms/step - loss: 0.2715 - mean_squared_error: 0.2715\n",
      "Epoch 2/3\n",
      "124/124 [==============================] - 86s 683ms/step - loss: 0.2464 - mean_squared_error: 0.2464\n",
      "Epoch 3/3\n",
      "124/124 [==============================] - 84s 674ms/step - loss: 0.2407 - mean_squared_error: 0.2407\n",
      "31/31 [==============================] - 21s 528ms/step - loss: 0.2516 - mean_squared_error: 0.2516\n",
      "Test loss: 0.2515675723552704\n",
      "Test MSE: 0.2515675723552704\n",
      "31/31 [==============================] - 22s 555ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(test_dataset\u001b[39m.\u001b[39mbatch(\u001b[39m16\u001b[39m))\n\u001b[1;32m     47\u001b[0m \u001b[39m# Calculate accuracy (you may need to define a threshold for each personality trait)\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m accuracy_cEXT \u001b[39m=\u001b[39m accuracy_score(test_traits[:, \u001b[39m0\u001b[39m], (predictions[:, \u001b[39m0\u001b[39;49m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m))\n\u001b[1;32m     49\u001b[0m accuracy_cNEU \u001b[39m=\u001b[39m accuracy_score(test_traits[:, \u001b[39m1\u001b[39m], (predictions[:, \u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m))\n\u001b[1;32m     50\u001b[0m accuracy_cAGR \u001b[39m=\u001b[39m accuracy_score(test_traits[:, \u001b[39m2\u001b[39m], (predictions[:, \u001b[39m2\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m))\n",
      "File \u001b[0;32m~/Downloads/Project-Video_copy/venv/lib/python3.9/site-packages/transformers/utils/generic.py:320\u001b[0m, in \u001b[0;36mModelOutput.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_dict[k]\n\u001b[1;32m    319\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto_tuple()[k]\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your pre-processed essays data\n",
    "essays = pickle.load(open(\"data/essays/essays2467.p\", \"rb\"))\n",
    "\n",
    "# Extract essay text and personality traits\n",
    "texts = [essay.clean_text for essay in essays]\n",
    "traits = np.array([[essay.cEXT, essay.cNEU, essay.cAGR, essay.cCON, essay.cOPN] for essay in essays])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_texts, test_texts, train_traits, test_traits = train_test_split(texts, traits, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-cased\")\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-large-cased\", num_labels=5)\n",
    "\n",
    "# Tokenize and encode your training and testing data\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors=\"tf\")\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, return_tensors=\"tf\")\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_traits))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_traits))\n",
    "\n",
    "# Define the training parameters\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "metric = tf.keras.metrics.MeanSquaredError()\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "results = model.evaluate(test_dataset.batch(16))\n",
    "print(\"Test loss:\", results[0])\n",
    "print(\"Test MSE:\", results[1])\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(test_dataset.batch(16))\n",
    "\n",
    "# Calculate accuracy (you may need to define a threshold for each personality trait)\n",
    "accuracy_cEXT = accuracy_score(test_traits[:, 0], (predictions[:, 0] >= 0.5).astype(int))\n",
    "accuracy_cNEU = accuracy_score(test_traits[:, 1], (predictions[:, 1] >= 0.5).astype(int))\n",
    "accuracy_cAGR = accuracy_score(test_traits[:, 2], (predictions[:, 2] >= 0.5).astype(int))\n",
    "accuracy_cCON = accuracy_score(test_traits[:, 3], (predictions[:, 3] >= 0.5).astype(int))\n",
    "accuracy_cOPN = accuracy_score(test_traits[:, 4], (predictions[:, 4] >= 0.5).astype(int))\n",
    "\n",
    "print(\"Accuracy for cEXT:\", accuracy_cEXT)\n",
    "print(\"Accuracy for cNEU:\", accuracy_cNEU)\n",
    "print(\"Accuracy for cAGR:\", accuracy_cAGR)\n",
    "print(\"Accuracy for cCON:\", accuracy_cCON)\n",
    "print(\"Accuracy for cOPN:\", accuracy_cOPN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_TensorSliceDataset element_spec=({'input_ids': TensorSpec(shape=(128,), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(128,), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(128,), dtype=tf.int32, name=None)}, TensorSpec(shape=(5,), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_TensorSliceDataset element_spec=({'input_ids': TensorSpec(shape=(128,), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(128,), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(128,), dtype=tf.int32, name=None)}, TensorSpec(shape=(5,), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
